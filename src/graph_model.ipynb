{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def83095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory cleaned: c:\\internship\\Prodify-V2.0\\src\\logs\n",
      "Logger initialized. All logs will be saved to: c:\\internship\\Prodify-V2.0\\src\\logs\\log.txt\n",
      "[2025-09-14 14:16:37] - logger:_log - INFO - Logger initialized. Logs will be saved to c:\\internship\\Prodify-V2.0\\src\\logs\\log.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 14:16:38] - warnings:_showwarnmsg - WARNING - C:\\Users\\os255022\\AppData\\Local\\Temp\\ipykernel_17548\\4279033757.py:1: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(FULL_DATASET_PATH)\n",
      "\n",
      "[2025-09-14 14:17:14] - SentenceTransformer:__init__ - INFO - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large-instruct\n",
      "[2025-09-14 14:17:14] - connectionpool:_new_conn - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "[2025-09-14 14:17:15] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/modules.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:16] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/modules.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:16] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/config_sentence_transformers.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:16] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:17] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/config_sentence_transformers.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:17] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:17] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/README.md HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:17] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/README.md HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:18] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/modules.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:18] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/modules.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:18] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_bert_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:18] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_roberta_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:19] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_distilbert_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:19] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_camembert_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:19] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_albert_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:20] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/sentence_xlm-roberta_config.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:20] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/sentence_xlm-roberta_config.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:20] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "[2025-09-14 14:17:21] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:21] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/config.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:22] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:22] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:22] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"GET /api/models/intfloat/multilingual-e5-large-instruct/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "[2025-09-14 14:17:23] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /intfloat/multilingual-e5-large-instruct/resolve/main/1_Pooling/config.json HTTP/1.1\" 307 0\n",
      "[2025-09-14 14:17:23] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/models/intfloat/multilingual-e5-large-instruct/274baa43b0e13e37fafa6428dbc7938e62e5c439/1_Pooling%2Fconfig.json HTTP/1.1\" 200 0\n",
      "[2025-09-14 14:17:24] - connectionpool:_make_request - DEBUG - https://huggingface.co:443 \"GET /api/models/intfloat/multilingual-e5-large-instruct HTTP/1.1\" 200 287688\n",
      "[2025-09-14 14:20:51] - logger:flush - INFO - \n",
      "Epoch 020 | Loss: 2.4943 | Train Acc: 0.5410 | Test Acc: 0.5433\n",
      "Epoch 040 | Loss: 1.4765 | Train Acc: 0.6666 | Test Acc: 0.6684\n",
      "Epoch 060 | Loss: 1.1607 | Train Acc: 0.7229 | Test Acc: 0.7239\n",
      "Epoch 080 | Loss: 0.8977 | Train Acc: 0.7619 | Test Acc: 0.7599\n",
      "Epoch 100 | Loss: 0.7168 | Train Acc: 0.7737 | Test Acc: 0.7720\n",
      "Epoch 120 | Loss: 0.5951 | Train Acc: 0.7572 | Test Acc: 0.7555\n",
      "Epoch 140 | Loss: 0.4980 | Train Acc: 0.7187 | Test Acc: 0.7154\n",
      "Epoch 160 | Loss: 0.4314 | Train Acc: 0.6620 | Test Acc: 0.6605\n",
      "Epoch 180 | Loss: 0.3676 | Train Acc: 0.6448 | Test Acc: 0.6433\n",
      "Epoch 200 | Loss: 0.3198 | Train Acc: 0.6470 | Test Acc: 0.6460\n",
      "Training finished. Best test acc:0.7719989493039139\n",
      "[2025-09-14 14:21:19] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.8070 | Train Acc: 0.5617 | Test Acc: 0.5673\n",
      "Epoch 060 | Loss: 1.1776 | Train Acc: 0.6724 | Test Acc: 0.6716\n",
      "Epoch 090 | Loss: 0.8168 | Train Acc: 0.7531 | Test Acc: 0.7512\n",
      "Epoch 120 | Loss: 0.6210 | Train Acc: 0.7975 | Test Acc: 0.7937\n",
      "Epoch 150 | Loss: 0.4868 | Train Acc: 0.7403 | Test Acc: 0.7406\n",
      "Epoch 180 | Loss: 0.3928 | Train Acc: 0.6648 | Test Acc: 0.6684\n",
      "Epoch 210 | Loss: 0.3238 | Train Acc: 0.6433 | Test Acc: 0.6471\n",
      "Epoch 240 | Loss: 0.2643 | Train Acc: 0.6316 | Test Acc: 0.6352\n",
      "Epoch 270 | Loss: 0.2213 | Train Acc: 0.6246 | Test Acc: 0.6290\n",
      "Epoch 300 | Loss: 0.1884 | Train Acc: 0.6075 | Test Acc: 0.6122\n",
      "Training finished. Best test acc:0.7937352245862884\n",
      "[2025-09-14 14:22:05] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.3876 | Train Acc: 0.6189 | Test Acc: 0.6195\n",
      "Epoch 060 | Loss: 0.8553 | Train Acc: 0.7299 | Test Acc: 0.7298\n",
      "Epoch 090 | Loss: 0.5689 | Train Acc: 0.7042 | Test Acc: 0.7026\n",
      "Epoch 120 | Loss: 0.4154 | Train Acc: 0.6657 | Test Acc: 0.6650\n",
      "Epoch 150 | Loss: 0.3053 | Train Acc: 0.6745 | Test Acc: 0.6765\n",
      "Epoch 180 | Loss: 0.2331 | Train Acc: 0.6579 | Test Acc: 0.6604\n",
      "Epoch 210 | Loss: 0.1832 | Train Acc: 0.6335 | Test Acc: 0.6367\n",
      "Epoch 240 | Loss: 0.1483 | Train Acc: 0.6132 | Test Acc: 0.6161\n",
      "Epoch 270 | Loss: 0.1196 | Train Acc: 0.6011 | Test Acc: 0.6019\n",
      "Epoch 300 | Loss: 0.0957 | Train Acc: 0.5982 | Test Acc: 0.6000\n",
      "Training finished. Best test acc:0.7298397688468611\n",
      "[2025-09-14 14:22:40] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.8296 | Train Acc: 0.5563 | Test Acc: 0.5602\n",
      "Epoch 060 | Loss: 1.2951 | Train Acc: 0.6061 | Test Acc: 0.6075\n",
      "Epoch 090 | Loss: 0.9578 | Train Acc: 0.6600 | Test Acc: 0.6581\n",
      "Epoch 120 | Loss: 0.6882 | Train Acc: 0.6800 | Test Acc: 0.6760\n",
      "Epoch 150 | Loss: 0.5425 | Train Acc: 0.6675 | Test Acc: 0.6637\n",
      "Epoch 180 | Loss: 0.4414 | Train Acc: 0.6731 | Test Acc: 0.6694\n",
      "Epoch 210 | Loss: 0.3665 | Train Acc: 0.6514 | Test Acc: 0.6520\n",
      "Epoch 240 | Loss: 0.3126 | Train Acc: 0.6356 | Test Acc: 0.6364\n",
      "Epoch 270 | Loss: 0.2588 | Train Acc: 0.6277 | Test Acc: 0.6296\n",
      "Epoch 300 | Loss: 0.2193 | Train Acc: 0.6317 | Test Acc: 0.6338\n",
      "Training finished. Best test acc:0.6759915944313107\n",
      "[2025-09-14 14:22:58] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.7640 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 060 | Loss: 1.0895 | Train Acc: 0.7217 | Test Acc: 0.7238\n",
      "Epoch 090 | Loss: 0.7503 | Train Acc: 0.7802 | Test Acc: 0.7773\n",
      "Epoch 120 | Loss: 0.5459 | Train Acc: 0.8155 | Test Acc: 0.8111\n",
      "Epoch 150 | Loss: 0.4275 | Train Acc: 0.8291 | Test Acc: 0.8253\n",
      "Epoch 180 | Loss: 0.3349 | Train Acc: 0.8086 | Test Acc: 0.8059\n",
      "Epoch 210 | Loss: 0.2678 | Train Acc: 0.7344 | Test Acc: 0.7331\n",
      "Epoch 240 | Loss: 0.2168 | Train Acc: 0.6623 | Test Acc: 0.6613\n",
      "Epoch 270 | Loss: 0.1824 | Train Acc: 0.6342 | Test Acc: 0.6327\n",
      "Epoch 300 | Loss: 0.1537 | Train Acc: 0.6176 | Test Acc: 0.6156\n",
      "Training finished. Best test acc:0.8252561071710008\n",
      "[2025-09-14 14:23:23] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 0.9034 | Train Acc: 0.6720 | Test Acc: 0.6684\n",
      "Epoch 060 | Loss: 0.4818 | Train Acc: 0.7434 | Test Acc: 0.7431\n",
      "Epoch 090 | Loss: 0.3053 | Train Acc: 0.7343 | Test Acc: 0.7315\n",
      "Epoch 120 | Loss: 0.2227 | Train Acc: 0.7352 | Test Acc: 0.7359\n",
      "Epoch 150 | Loss: 0.1660 | Train Acc: 0.7225 | Test Acc: 0.7226\n",
      "Epoch 180 | Loss: 0.1242 | Train Acc: 0.7344 | Test Acc: 0.7331\n",
      "Epoch 210 | Loss: 0.0915 | Train Acc: 0.7218 | Test Acc: 0.7206\n",
      "Epoch 240 | Loss: 0.0842 | Train Acc: 0.7242 | Test Acc: 0.7240\n",
      "Epoch 270 | Loss: 0.0521 | Train Acc: 0.6519 | Test Acc: 0.6505\n",
      "Epoch 300 | Loss: 0.0358 | Train Acc: 0.6137 | Test Acc: 0.6135\n",
      "Training finished. Best test acc:0.7431048069345941\n",
      "[2025-09-14 14:23:42] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 5.2167 | Train Acc: 0.4064 | Test Acc: 0.4114\n",
      "Epoch 060 | Loss: 4.6221 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 090 | Loss: 3.9178 | Train Acc: 0.5694 | Test Acc: 0.5745\n",
      "Epoch 120 | Loss: 3.1618 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 150 | Loss: 2.4910 | Train Acc: 0.5788 | Test Acc: 0.5835\n",
      "Epoch 180 | Loss: 2.0630 | Train Acc: 0.5956 | Test Acc: 0.5991\n",
      "Epoch 210 | Loss: 1.8066 | Train Acc: 0.5853 | Test Acc: 0.5919\n",
      "Epoch 240 | Loss: 1.6300 | Train Acc: 0.5836 | Test Acc: 0.5882\n",
      "Epoch 270 | Loss: 1.4957 | Train Acc: 0.6297 | Test Acc: 0.6313\n",
      "Epoch 300 | Loss: 1.3855 | Train Acc: 0.7148 | Test Acc: 0.7163\n",
      "Training finished. Best test acc:0.7163120567375887\n",
      "[2025-09-14 14:24:01] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.7223 | Train Acc: 0.5963 | Test Acc: 0.6034\n",
      "Epoch 060 | Loss: 1.1010 | Train Acc: 0.7093 | Test Acc: 0.7107\n",
      "Epoch 090 | Loss: 0.7504 | Train Acc: 0.7748 | Test Acc: 0.7720\n",
      "Epoch 120 | Loss: 0.5555 | Train Acc: 0.7921 | Test Acc: 0.7893\n",
      "Epoch 150 | Loss: 0.4362 | Train Acc: 0.7873 | Test Acc: 0.7844\n",
      "Epoch 180 | Loss: 0.3482 | Train Acc: 0.7789 | Test Acc: 0.7769\n",
      "Epoch 210 | Loss: 0.2804 | Train Acc: 0.7411 | Test Acc: 0.7408\n",
      "Epoch 240 | Loss: 0.2314 | Train Acc: 0.7347 | Test Acc: 0.7338\n",
      "Epoch 270 | Loss: 0.1931 | Train Acc: 0.7089 | Test Acc: 0.7107\n",
      "Epoch 300 | Loss: 0.1662 | Train Acc: 0.6986 | Test Acc: 0.7019\n",
      "Training finished. Best test acc:0.7892697662201208\n",
      "[2025-09-14 14:24:38] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.7776 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 060 | Loss: 1.1815 | Train Acc: 0.6858 | Test Acc: 0.6878\n",
      "Epoch 090 | Loss: 0.8141 | Train Acc: 0.7634 | Test Acc: 0.7654\n",
      "Epoch 120 | Loss: 0.6149 | Train Acc: 0.7963 | Test Acc: 0.7916\n",
      "Epoch 150 | Loss: 0.4947 | Train Acc: 0.7716 | Test Acc: 0.7700\n",
      "Epoch 180 | Loss: 0.4009 | Train Acc: 0.7076 | Test Acc: 0.7102\n",
      "Epoch 210 | Loss: 0.3253 | Train Acc: 0.6831 | Test Acc: 0.6859\n",
      "Epoch 240 | Loss: 0.2694 | Train Acc: 0.6947 | Test Acc: 0.6973\n",
      "Epoch 270 | Loss: 0.2235 | Train Acc: 0.6987 | Test Acc: 0.7002\n",
      "Epoch 300 | Loss: 0.1898 | Train Acc: 0.6818 | Test Acc: 0.6858\n",
      "Training finished. Best test acc:0.7915681639085894\n",
      "[2025-09-14 14:25:00] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.7593 | Train Acc: 0.5501 | Test Acc: 0.5555\n",
      "Epoch 060 | Loss: 1.1246 | Train Acc: 0.6977 | Test Acc: 0.6983\n",
      "Epoch 090 | Loss: 0.7577 | Train Acc: 0.7792 | Test Acc: 0.7740\n",
      "Epoch 120 | Loss: 0.5535 | Train Acc: 0.8179 | Test Acc: 0.8111\n",
      "Epoch 150 | Loss: 0.4358 | Train Acc: 0.8269 | Test Acc: 0.8210\n",
      "Epoch 180 | Loss: 0.3477 | Train Acc: 0.7925 | Test Acc: 0.7893\n",
      "Epoch 210 | Loss: 0.2737 | Train Acc: 0.7617 | Test Acc: 0.7564\n",
      "Epoch 240 | Loss: 0.2252 | Train Acc: 0.7195 | Test Acc: 0.7149\n",
      "Epoch 270 | Loss: 0.1867 | Train Acc: 0.6842 | Test Acc: 0.6821\n",
      "Epoch 300 | Loss: 0.1587 | Train Acc: 0.6590 | Test Acc: 0.6583\n",
      "Training finished. Best test acc:0.8209876543209876\n",
      "[2025-09-14 14:25:45] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.4304 | Train Acc: 0.5623 | Test Acc: 0.5656\n",
      "Epoch 060 | Loss: 0.8746 | Train Acc: 0.7501 | Test Acc: 0.7489\n",
      "Epoch 090 | Loss: 0.5868 | Train Acc: 0.8091 | Test Acc: 0.8019\n",
      "Epoch 120 | Loss: 0.4343 | Train Acc: 0.8277 | Test Acc: 0.8237\n",
      "Epoch 150 | Loss: 0.3194 | Train Acc: 0.8310 | Test Acc: 0.8225\n",
      "Epoch 180 | Loss: 0.2401 | Train Acc: 0.8292 | Test Acc: 0.8250\n",
      "Epoch 210 | Loss: 0.1873 | Train Acc: 0.8103 | Test Acc: 0.8082\n",
      "Epoch 240 | Loss: 0.1519 | Train Acc: 0.7587 | Test Acc: 0.7562\n",
      "Epoch 270 | Loss: 0.1211 | Train Acc: 0.7119 | Test Acc: 0.7129\n",
      "Epoch 300 | Loss: 0.0944 | Train Acc: 0.6756 | Test Acc: 0.6749\n",
      "Training finished. Best test acc:0.8249934331494615\n",
      "[2025-09-14 14:26:19] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.3247 | Train Acc: 0.5823 | Test Acc: 0.5838\n",
      "Epoch 060 | Loss: 0.7623 | Train Acc: 0.7445 | Test Acc: 0.7424\n",
      "Epoch 090 | Loss: 0.5139 | Train Acc: 0.7740 | Test Acc: 0.7688\n",
      "Epoch 120 | Loss: 0.3574 | Train Acc: 0.7476 | Test Acc: 0.7430\n",
      "Epoch 150 | Loss: 0.2604 | Train Acc: 0.7097 | Test Acc: 0.7069\n",
      "Epoch 180 | Loss: 0.2028 | Train Acc: 0.6670 | Test Acc: 0.6679\n",
      "Epoch 210 | Loss: 0.1618 | Train Acc: 0.6407 | Test Acc: 0.6414\n",
      "Epoch 240 | Loss: 0.1303 | Train Acc: 0.6209 | Test Acc: 0.6210\n",
      "Epoch 270 | Loss: 0.1011 | Train Acc: 0.6016 | Test Acc: 0.6033\n",
      "Epoch 300 | Loss: 0.0775 | Train Acc: 0.5898 | Test Acc: 0.5911\n",
      "Training finished. Best test acc:0.7688468610454426\n",
      "[2025-09-14 14:27:25] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.0787 | Train Acc: 0.6932 | Test Acc: 0.6915\n",
      "Epoch 060 | Loss: 0.6074 | Train Acc: 0.7607 | Test Acc: 0.7591\n",
      "Epoch 090 | Loss: 0.3936 | Train Acc: 0.7783 | Test Acc: 0.7784\n",
      "Epoch 120 | Loss: 0.2731 | Train Acc: 0.7593 | Test Acc: 0.7564\n",
      "Epoch 150 | Loss: 0.2030 | Train Acc: 0.7647 | Test Acc: 0.7617\n",
      "Epoch 180 | Loss: 0.1591 | Train Acc: 0.7481 | Test Acc: 0.7470\n",
      "Epoch 210 | Loss: 0.1209 | Train Acc: 0.7330 | Test Acc: 0.7314\n",
      "Epoch 240 | Loss: 0.0934 | Train Acc: 0.7305 | Test Acc: 0.7300\n",
      "Epoch 270 | Loss: 0.0677 | Train Acc: 0.7214 | Test Acc: 0.7191\n",
      "Epoch 300 | Loss: 0.0592 | Train Acc: 0.7068 | Test Acc: 0.7034\n",
      "Training finished. Best test acc:0.7783687943262412\n",
      "[2025-09-14 14:29:21] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.2151 | Train Acc: 0.6317 | Test Acc: 0.6304\n",
      "Epoch 060 | Loss: 0.7077 | Train Acc: 0.6474 | Test Acc: 0.6439\n",
      "Epoch 090 | Loss: 0.4737 | Train Acc: 0.6740 | Test Acc: 0.6737\n",
      "Epoch 120 | Loss: 0.3317 | Train Acc: 0.6763 | Test Acc: 0.6748\n",
      "Epoch 150 | Loss: 0.2442 | Train Acc: 0.6872 | Test Acc: 0.6873\n",
      "Epoch 180 | Loss: 0.1880 | Train Acc: 0.6901 | Test Acc: 0.6898\n",
      "Epoch 210 | Loss: 0.1510 | Train Acc: 0.6569 | Test Acc: 0.6587\n",
      "Epoch 240 | Loss: 0.1174 | Train Acc: 0.6503 | Test Acc: 0.6525\n",
      "Epoch 270 | Loss: 0.0936 | Train Acc: 0.6551 | Test Acc: 0.6560\n",
      "Epoch 300 | Loss: 0.0637 | Train Acc: 0.6325 | Test Acc: 0.6356\n",
      "Training finished. Best test acc:0.6898476490675072\n",
      "[2025-09-14 14:30:06] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.4335 | Train Acc: 0.5694 | Test Acc: 0.5745\n",
      "Epoch 060 | Loss: 0.8487 | Train Acc: 0.7613 | Test Acc: 0.7587\n",
      "Epoch 090 | Loss: 0.5841 | Train Acc: 0.7929 | Test Acc: 0.7884\n",
      "Epoch 120 | Loss: 0.4235 | Train Acc: 0.8086 | Test Acc: 0.8051\n",
      "Epoch 150 | Loss: 0.3140 | Train Acc: 0.8085 | Test Acc: 0.8015\n",
      "Epoch 180 | Loss: 0.2412 | Train Acc: 0.7699 | Test Acc: 0.7680\n",
      "Epoch 210 | Loss: 0.1935 | Train Acc: 0.7871 | Test Acc: 0.7857\n",
      "Epoch 240 | Loss: 0.1571 | Train Acc: 0.7386 | Test Acc: 0.7373\n",
      "Epoch 270 | Loss: 0.1277 | Train Acc: 0.7044 | Test Acc: 0.7031\n",
      "Epoch 300 | Loss: 0.1024 | Train Acc: 0.6617 | Test Acc: 0.6611\n",
      "Training finished. Best test acc:0.8050958760178618\n",
      "[2025-09-14 14:30:38] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.4209 | Train Acc: 0.5694 | Test Acc: 0.5745\n",
      "Epoch 060 | Loss: 0.8400 | Train Acc: 0.7542 | Test Acc: 0.7541\n",
      "Epoch 090 | Loss: 0.5594 | Train Acc: 0.7934 | Test Acc: 0.7906\n",
      "Epoch 120 | Loss: 0.4108 | Train Acc: 0.8065 | Test Acc: 0.8023\n",
      "Epoch 150 | Loss: 0.3116 | Train Acc: 0.7736 | Test Acc: 0.7750\n",
      "Epoch 180 | Loss: 0.2450 | Train Acc: 0.7223 | Test Acc: 0.7214\n",
      "Epoch 210 | Loss: 0.1978 | Train Acc: 0.6990 | Test Acc: 0.6999\n",
      "Epoch 240 | Loss: 0.1614 | Train Acc: 0.6830 | Test Acc: 0.6868\n",
      "Epoch 270 | Loss: 0.1275 | Train Acc: 0.6699 | Test Acc: 0.6707\n",
      "Epoch 300 | Loss: 0.1014 | Train Acc: 0.6832 | Test Acc: 0.6861\n",
      "Training finished. Best test acc:0.8023377987916995\n",
      "[2025-09-14 14:31:10] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 0.9186 | Train Acc: 0.6271 | Test Acc: 0.6311\n",
      "Epoch 060 | Loss: 0.5214 | Train Acc: 0.6776 | Test Acc: 0.6792\n",
      "Epoch 090 | Loss: 0.3500 | Train Acc: 0.6604 | Test Acc: 0.6635\n",
      "Epoch 120 | Loss: 0.2582 | Train Acc: 0.5952 | Test Acc: 0.5962\n",
      "Epoch 150 | Loss: 0.1916 | Train Acc: 0.5934 | Test Acc: 0.5972\n",
      "Epoch 180 | Loss: 0.1404 | Train Acc: 0.5431 | Test Acc: 0.5490\n",
      "Epoch 210 | Loss: 0.1081 | Train Acc: 0.5453 | Test Acc: 0.5515\n",
      "Epoch 240 | Loss: 0.0926 | Train Acc: 0.4985 | Test Acc: 0.5030\n",
      "Epoch 270 | Loss: 0.0695 | Train Acc: 0.5133 | Test Acc: 0.5183\n",
      "Epoch 300 | Loss: 0.0420 | Train Acc: 0.4995 | Test Acc: 0.5039\n",
      "Training finished. Best test acc:0.6792093511951668\n",
      "[2025-09-14 14:31:45] - logger:flush - INFO - \n",
      "Epoch 020 | Loss: 1.7884 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 040 | Loss: 1.2902 | Train Acc: 0.6020 | Test Acc: 0.6049\n",
      "Epoch 060 | Loss: 0.9514 | Train Acc: 0.7180 | Test Acc: 0.7174\n",
      "Epoch 080 | Loss: 0.7274 | Train Acc: 0.7042 | Test Acc: 0.7053\n",
      "Epoch 100 | Loss: 0.5758 | Train Acc: 0.7377 | Test Acc: 0.7347\n",
      "Epoch 120 | Loss: 0.4621 | Train Acc: 0.7599 | Test Acc: 0.7546\n",
      "Epoch 140 | Loss: 0.3869 | Train Acc: 0.7607 | Test Acc: 0.7567\n",
      "Epoch 160 | Loss: 0.3295 | Train Acc: 0.7659 | Test Acc: 0.7625\n",
      "Epoch 180 | Loss: 0.2728 | Train Acc: 0.7643 | Test Acc: 0.7594\n",
      "Epoch 200 | Loss: 0.2386 | Train Acc: 0.7390 | Test Acc: 0.7367\n",
      "Training finished. Best test acc:0.7625426845285002\n",
      "[2025-09-14 14:32:07] - logger:flush - INFO - \n",
      "Epoch 020 | Loss: 1.8074 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 040 | Loss: 1.2387 | Train Acc: 0.6582 | Test Acc: 0.6577\n",
      "Epoch 060 | Loss: 0.8947 | Train Acc: 0.7554 | Test Acc: 0.7548\n",
      "Epoch 080 | Loss: 0.6685 | Train Acc: 0.7659 | Test Acc: 0.7648\n",
      "Epoch 100 | Loss: 0.5262 | Train Acc: 0.7802 | Test Acc: 0.7780\n",
      "Epoch 120 | Loss: 0.4240 | Train Acc: 0.7888 | Test Acc: 0.7878\n",
      "Epoch 140 | Loss: 0.3519 | Train Acc: 0.7852 | Test Acc: 0.7824\n",
      "Epoch 160 | Loss: 0.2947 | Train Acc: 0.7804 | Test Acc: 0.7804\n",
      "Epoch 180 | Loss: 0.2500 | Train Acc: 0.7432 | Test Acc: 0.7445\n",
      "Epoch 200 | Loss: 0.2113 | Train Acc: 0.7220 | Test Acc: 0.7261\n",
      "Training finished. Best test acc:0.78775939059627\n",
      "[2025-09-14 14:32:48] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.4932 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 060 | Loss: 0.9561 | Train Acc: 0.7159 | Test Acc: 0.7141\n",
      "Epoch 090 | Loss: 0.6293 | Train Acc: 0.7592 | Test Acc: 0.7552\n",
      "Epoch 120 | Loss: 0.4535 | Train Acc: 0.7883 | Test Acc: 0.7869\n",
      "Epoch 150 | Loss: 0.3389 | Train Acc: 0.7897 | Test Acc: 0.7889\n",
      "Epoch 180 | Loss: 0.2613 | Train Acc: 0.7024 | Test Acc: 0.7031\n",
      "Epoch 210 | Loss: 0.2076 | Train Acc: 0.5629 | Test Acc: 0.5660\n",
      "Epoch 240 | Loss: 0.1691 | Train Acc: 0.5062 | Test Acc: 0.5079\n",
      "Epoch 270 | Loss: 0.1380 | Train Acc: 0.4923 | Test Acc: 0.4925\n",
      "Epoch 300 | Loss: 0.1115 | Train Acc: 0.4889 | Test Acc: 0.4891\n",
      "Training finished. Best test acc:0.788875755187812\n",
      "[2025-09-14 14:33:26] - logger:flush - INFO - \n",
      "Epoch 030 | Loss: 1.5349 | Train Acc: 0.5647 | Test Acc: 0.5705\n",
      "Epoch 060 | Loss: 1.0118 | Train Acc: 0.7183 | Test Acc: 0.7177\n",
      "Epoch 090 | Loss: 0.6389 | Train Acc: 0.7398 | Test Acc: 0.7388\n",
      "Epoch 120 | Loss: 0.4655 | Train Acc: 0.7689 | Test Acc: 0.7679\n",
      "Epoch 150 | Loss: 0.3541 | Train Acc: 0.7764 | Test Acc: 0.7768\n",
      "Epoch 180 | Loss: 0.2690 | Train Acc: 0.7591 | Test Acc: 0.7593\n",
      "Epoch 210 | Loss: 0.2138 | Train Acc: 0.7526 | Test Acc: 0.7507\n",
      "Epoch 240 | Loss: 0.1741 | Train Acc: 0.6694 | Test Acc: 0.6712\n",
      "Epoch 270 | Loss: 0.1415 | Train Acc: 0.6521 | Test Acc: 0.6553\n",
      "Epoch 300 | Loss: 0.1155 | Train Acc: 0.6288 | Test Acc: 0.6318\n",
      "Training finished. Best test acc:0.7767927501970056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HANConv, GATConv\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import load_embedding_model\n",
    "from constants import FULL_DATASET_PATH, E5_LARGE_INSTRUCT_CONFIG_PATH, RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c494ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "torch.cuda.manual_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605f15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FULL_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11dc21b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178703"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89bb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"class\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e433f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_seg = LabelEncoder()\n",
    "le_fam = LabelEncoder()\n",
    "le_cls = LabelEncoder()\n",
    "\n",
    "df[\"segment_encode\"] = le_seg.fit_transform(df[\"segment\"])\n",
    "df[\"family_encode\"]  = le_fam.fit_transform(df[\"family\"])\n",
    "df[\"class_encode\"]   = le_cls.fit_transform(df[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2052eb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212f1d525c624430b9a03a198fc005a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_model = load_embedding_model(E5_LARGE_INSTRUCT_CONFIG_PATH)\n",
    "\n",
    "product_embeds = embed_model.get_embeddings(df[\"product_name\"].tolist())\n",
    "product_embeds = torch.tensor(product_embeds, dtype=torch.float32)  # shape [N_products, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46cad0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_y = torch.tensor(df[\"segment_encode\"].values, dtype=torch.long)  # shape [N_products]\n",
    "family_y = torch.tensor(df[\"family_encode\"].values, dtype=torch.long)  # shape [N_products]\n",
    "class_y = torch.tensor(df[\"class_encode\"].values, dtype=torch.long)  # shape [N_products]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8b0056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(len(df)))\n",
    "train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42)\n",
    "\n",
    "train_mask = torch.zeros(len(df), dtype=torch.bool)\n",
    "test_mask  = torch.zeros(len(df), dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx]  = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74421f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_products = len(df)\n",
    "num_segments = df[\"segment_encode\"].nunique()\n",
    "num_families = df[\"family_encode\"].nunique()\n",
    "num_classes  = df[\"class_encode\"].nunique()\n",
    "\n",
    "# --- Product to Segment edges ---\n",
    "prod_to_seg_src = torch.arange(num_products, dtype=torch.long)\n",
    "prod_to_seg_dst = torch.tensor(df[\"segment_encode\"].values, dtype=torch.long)\n",
    "prod_to_seg_edge_index = torch.stack([prod_to_seg_src, prod_to_seg_dst], dim=0)\n",
    "\n",
    "# --- Segment to Family edges ---\n",
    "seg_to_fam_src = torch.tensor(df[\"segment_encode\"].values, dtype=torch.long)\n",
    "seg_to_fam_dst = torch.tensor(df[\"family_encode\"].values, dtype=torch.long)\n",
    "seg_to_fam_edge_index = torch.stack([seg_to_fam_src, seg_to_fam_dst], dim=0)\n",
    "\n",
    "# --- Family to Class edges ---\n",
    "fam_to_cls_src = torch.tensor(df[\"family_encode\"].values, dtype=torch.long)\n",
    "fam_to_cls_dst = torch.tensor(df[\"class_encode\"].values, dtype=torch.long)\n",
    "fam_to_cls_edge_index = torch.stack([fam_to_cls_src, fam_to_cls_dst], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26aac3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "\n",
    "# Product nodes\n",
    "data[\"product\"].x = product_embeds\n",
    "# data[\"product\"].segment = segment_y\n",
    "# data[\"product\"].family = family_y\n",
    "# data[\"product\"]._class = class_y\n",
    "data[\"product\"].train_mask = train_mask\n",
    "data[\"product\"].test_mask = test_mask\n",
    "data[\"product\"].y = class_y\n",
    "\n",
    "# Segment / Family / Class nodes don’t have features (yet)\n",
    "# They’ll get embeddings via the NodeFeatureEncoder\n",
    "data[\"segment\"].num_nodes = num_segments\n",
    "data[\"family\"].num_nodes  = num_families\n",
    "data[\"class\"].num_nodes   = num_classes\n",
    "\n",
    "# Edges\n",
    "data[\"product\", \"to\", \"segment\"].edge_index = prod_to_seg_edge_index\n",
    "data[\"segment\", \"to\", \"family\"].edge_index  = seg_to_fam_edge_index\n",
    "data[\"family\", \"to\", \"class\"].edge_index    = fam_to_cls_edge_index\n",
    "\n",
    "# Reverse edges\n",
    "data[\"segment\", \"rev_to\", \"product\"].edge_index = prod_to_seg_edge_index.flip(0)\n",
    "data[\"family\", \"rev_to\", \"segment\"].edge_index  = seg_to_fam_edge_index.flip(0)\n",
    "data[\"class\", \"rev_to\", \"family\"].edge_index    = fam_to_cls_edge_index.flip(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4030f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# -------------------------\n",
    "# NodeFeatureEncoder (same as before)\n",
    "# -------------------------\n",
    "class NodeFeatureEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        prod_in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_families: Optional[int] = None,\n",
    "        num_segments: Optional[int] = None,\n",
    "        num_classes: Optional[int] = None,\n",
    "        pretrained_category_embeddings: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.product_proj = nn.Linear(prod_in_dim, hidden_dim)\n",
    "        self.pretrained = pretrained_category_embeddings or {}\n",
    "\n",
    "        if 'segment' in self.pretrained:\n",
    "            self.register_buffer('segment_pre', self.pretrained['segment'])\n",
    "            self.segment_embedding = None\n",
    "            seg_in_dim = self.pretrained['segment'].shape[1]\n",
    "        else:\n",
    "            assert num_segments is not None, \"num_segments required if no pretrained segment embeddings\"\n",
    "            seg_in_dim = hidden_dim\n",
    "            self.segment_embedding = nn.Embedding(num_segments, seg_in_dim)\n",
    "\n",
    "        if 'family' in self.pretrained:\n",
    "            self.register_buffer('family_pre', self.pretrained['family'])\n",
    "            self.family_embedding = None\n",
    "            fam_in_dim = self.pretrained['family'].shape[1]\n",
    "        else:\n",
    "            assert num_families is not None, \"num_families required if no pretrained family embeddings\"\n",
    "            fam_in_dim = hidden_dim\n",
    "            self.family_embedding = nn.Embedding(num_families, fam_in_dim)\n",
    "\n",
    "        if 'class' in self.pretrained:\n",
    "            self.register_buffer('class_pre', self.pretrained['class'])\n",
    "            self.class_embedding = None\n",
    "            class_in_dim = self.pretrained['class'].shape[1]\n",
    "        else:\n",
    "            assert num_classes is not None, \"num_classes required if no pretrained class embeddings\"\n",
    "            class_in_dim = hidden_dim\n",
    "            self.class_embedding = nn.Embedding(num_classes, class_in_dim)\n",
    "\n",
    "        # project category dims -> hidden_dim\n",
    "        self.segment_proj = nn.Linear(seg_in_dim, hidden_dim)\n",
    "        self.family_proj  = nn.Linear(fam_in_dim, hidden_dim)\n",
    "        self.class_proj   = nn.Linear(class_in_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, product_x, segment_idx_or_none=None, family_idx_or_none=None, class_idx_or_none=None):\n",
    "        out = {}\n",
    "        out['product'] = self.product_proj(product_x)\n",
    "\n",
    "        if hasattr(self, 'segment_pre') and self.segment_pre is not None:\n",
    "            seg_feats = self.segment_pre\n",
    "        else:\n",
    "            seg_feats = self.segment_embedding(segment_idx_or_none)\n",
    "        out['segment'] = self.segment_proj(seg_feats)\n",
    "\n",
    "        if hasattr(self, 'family_pre') and self.family_pre is not None:\n",
    "            fam_feats = self.family_pre\n",
    "        else:\n",
    "            fam_feats = self.family_embedding(family_idx_or_none)\n",
    "        out['family'] = self.family_proj(fam_feats)\n",
    "\n",
    "        if hasattr(self, 'class_pre') and self.class_pre is not None:\n",
    "            class_feats = self.class_pre\n",
    "        else:\n",
    "            class_feats = self.class_embedding(class_idx_or_none)\n",
    "        out['class'] = self.class_proj(class_feats)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Hetero GNN with updated relations\n",
    "# -------------------------\n",
    "class HeteroSAGENet(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, out_classes: int, num_layers: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                # forward relations (updated)\n",
    "                ('product', 'to', 'segment'): SAGEConv((-1, -1), hidden_dim),\n",
    "                ('segment', 'to', 'family'): SAGEConv((-1, -1), hidden_dim),\n",
    "                ('family', 'to', 'class'): SAGEConv((-1, -1), hidden_dim),\n",
    "                # reverse relations\n",
    "                ('segment', 'rev_to', 'product'): SAGEConv((-1, -1), hidden_dim),\n",
    "                ('family', 'rev_to', 'segment'): SAGEConv((-1, -1), hidden_dim),\n",
    "                ('class', 'rev_to', 'family'): SAGEConv((-1, -1), hidden_dim),\n",
    "            }, aggr='mean')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.bn_product = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_segment = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_family  = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_class   = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x = x_dict\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index_dict)\n",
    "            x['product'] = F.relu(self.bn_product(x['product']))\n",
    "            x['segment'] = F.relu(self.bn_segment(x['segment']))\n",
    "            x['family']  = F.relu(self.bn_family(x['family']))\n",
    "            x['class']   = F.relu(self.bn_class(x['class']))\n",
    "\n",
    "            x['product'] = F.dropout(x['product'], p=self.dropout, training=self.training)\n",
    "            x['segment'] = F.dropout(x['segment'], p=self.dropout, training=self.training)\n",
    "            x['family']  = F.dropout(x['family'], p=self.dropout, training=self.training)\n",
    "            x['class']   = F.dropout(x['class'], p=self.dropout, training=self.training)\n",
    "\n",
    "        logits = self.classifier(x['product'])\n",
    "        return logits, x\n",
    "    \n",
    "class HeteroAttnNet(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, out_classes: int, num_layers: int = 2, dropout: float = 0.1, heads: int = 2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                # forward relations\n",
    "                ('product', 'to', 'segment'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "                ('segment', 'to', 'family'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "                ('family', 'to', 'class'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "                # reverse relations\n",
    "                ('segment', 'rev_to', 'product'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "                ('family', 'rev_to', 'segment'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "                ('class', 'rev_to', 'family'): GATConv(\n",
    "                    (-1, -1), hidden_dim, heads=heads, concat=False, add_self_loops=False\n",
    "                ),\n",
    "            }, aggr='mean')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        # batchnorms\n",
    "        self.bn_product = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_segment = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_family  = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn_class   = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x = x_dict\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index_dict)\n",
    "\n",
    "            x['product'] = F.relu(self.bn_product(x['product']))\n",
    "            x['segment'] = F.relu(self.bn_segment(x['segment']))\n",
    "            x['family']  = F.relu(self.bn_family(x['family']))\n",
    "            x['class']   = F.relu(self.bn_class(x['class']))\n",
    "\n",
    "            x['product'] = F.dropout(x['product'], p=self.dropout, training=self.training)\n",
    "            x['segment'] = F.dropout(x['segment'], p=self.dropout, training=self.training)\n",
    "            x['family']  = F.dropout(x['family'], p=self.dropout, training=self.training)\n",
    "            x['class']   = F.dropout(x['class'], p=self.dropout, training=self.training)\n",
    "\n",
    "        logits = self.classifier(x['product'])\n",
    "        return logits, x\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training wrapper (updated edge names/order)\n",
    "# -------------------------\n",
    "def train_model(\n",
    "    product_embeddings,                # tensor [N_p, 1024]\n",
    "    product_to_segment_edge_index,     # long tensor [2, E_ps]\n",
    "    segment_to_family_edge_index,      # long tensor [2, E_sf]\n",
    "    family_to_class_edge_index,        # long tensor [2, E_fc]\n",
    "    product_y,                         # long tensor [N_p]\n",
    "    product_train_mask,                # bool tensor [N_p]\n",
    "    product_test_mask,                 # bool tensor [N_p]\n",
    "    num_families=None,\n",
    "    num_segments=None,\n",
    "    num_classes=None,\n",
    "    pretrained_category_embeddings: Optional[Dict[str, torch.Tensor]] = None,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    heads=None,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    epochs=100,\n",
    "    verbose=True\n",
    "):\n",
    "    data = HeteroData()\n",
    "    data['product'].x = product_embeddings\n",
    "    data['product'].y = product_y\n",
    "    data['product'].train_mask = product_train_mask\n",
    "    data['product'].test_mask = product_test_mask\n",
    "\n",
    "    # Updated edges\n",
    "    data['product', 'to', 'segment'].edge_index = product_to_segment_edge_index\n",
    "    data['segment', 'to', 'family'].edge_index = segment_to_family_edge_index\n",
    "    data['family', 'to', 'class'].edge_index = family_to_class_edge_index\n",
    "\n",
    "    # reverse edges (flip rows)\n",
    "    data['segment', 'rev_to', 'product'].edge_index = product_to_segment_edge_index.flip(0)\n",
    "    data['family', 'rev_to', 'segment'].edge_index = segment_to_family_edge_index.flip(0)\n",
    "    data['class', 'rev_to', 'family'].edge_index = family_to_class_edge_index.flip(0)\n",
    "\n",
    "    encoder = NodeFeatureEncoder(\n",
    "        prod_in_dim=product_embeddings.size(1),\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_families=num_families,\n",
    "        num_segments=num_segments,\n",
    "        num_classes=num_classes,\n",
    "        pretrained_category_embeddings=pretrained_category_embeddings\n",
    "    ).to(device)\n",
    "\n",
    "    model = None\n",
    "    if heads is not None:\n",
    "        model = HeteroAttnNet(hidden_dim=hidden_dim, out_classes=num_classes, num_layers=num_layers, heads=heads).to(device)\n",
    "    else: \n",
    "        model = HeteroSAGENet(hidden_dim=hidden_dim, out_classes=num_classes, num_layers=num_layers).to(device)\n",
    "    \n",
    "    params = list(encoder.parameters()) + list(model.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # category indices for embedding path (if needed)\n",
    "    segment_idx = torch.arange(num_segments, dtype=torch.long, device=device) if (pretrained_category_embeddings is None or 'segment' not in (pretrained_category_embeddings or {})) else None\n",
    "    family_idx  = torch.arange(num_families, dtype=torch.long, device=device)  if (pretrained_category_embeddings is None or 'family' not in (pretrained_category_embeddings or {})) else None\n",
    "    class_idx   = torch.arange(num_classes, dtype=torch.long, device=device)   if (pretrained_category_embeddings is None or 'class' not in (pretrained_category_embeddings or {})) else None\n",
    "\n",
    "    data = data.to(device)\n",
    "    product_embeddings = product_embeddings.to(device)\n",
    "    product_y = product_y.to(device)\n",
    "    product_train_mask = product_train_mask.to(device)\n",
    "    product_test_mask = product_test_mask.to(device)\n",
    "\n",
    "    best_test_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        encoder.train(); model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_dict = encoder(product_embeddings, segment_idx, family_idx, class_idx)\n",
    "        logits, _ = model(x_dict, data.edge_index_dict)\n",
    "\n",
    "        loss = F.cross_entropy(logits[product_train_mask], product_y[product_train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose and epoch % max(1, epochs//10) == 0:\n",
    "            encoder.eval(); model.eval()\n",
    "            with torch.inference_mode():\n",
    "                x_eval = encoder(product_embeddings, segment_idx, family_idx, class_idx)\n",
    "                logits_eval, _ = model(x_eval, data.edge_index_dict)\n",
    "                pred = logits_eval.argmax(dim=-1)\n",
    "                train_acc = (pred[product_train_mask] == product_y[product_train_mask]).sum().item() / max(1, int(product_train_mask.sum().item()))\n",
    "                test_acc = (pred[product_test_mask] == product_y[product_test_mask]).sum().item() / max(1, int(product_test_mask.sum().item()))\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                best_state = {'encoder': encoder.state_dict(), 'model': model.state_dict(), 'epoch': epoch, 'test_acc': test_acc}\n",
    "            print(f\"\\nEpoch {epoch:03d} | Loss: {loss.item():.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining finished. Best test acc:\", best_test_acc)\n",
    "    return encoder, model, best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b07517ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.reset_accumulated_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad67ddbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb8c4b61b61447b8b3d59636b74808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder, model, best_state = train_model(\n",
    "    product_embeddings=data[\"product\"].x,                  # [N_products, 1024]\n",
    "    product_to_segment_edge_index=data[\"product\", \"to\", \"segment\"].edge_index,\n",
    "    segment_to_family_edge_index=data[\"segment\", \"to\", \"family\"].edge_index,\n",
    "    family_to_class_edge_index=data[\"family\", \"to\", \"class\"].edge_index,\n",
    "    product_y=data[\"product\"].y,                           # class labels\n",
    "    product_train_mask=data[\"product\"].train_mask,         # boolean mask\n",
    "    product_test_mask=data[\"product\"].test_mask,           # boolean mask\n",
    "    num_segments=data[\"segment\"].num_nodes,\n",
    "    num_families=data[\"family\"].num_nodes,\n",
    "    num_classes=data[\"class\"].num_nodes,\n",
    "    hidden_dim=200,       # you can tune\n",
    "    num_layers=3,         # number of GNN layers\n",
    "    # heads=2,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    epochs=300,            # train longer for better results\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fca74824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 0.7767927501970056)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_state[\"epoch\"], best_state[\"test_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba67010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prodify-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
